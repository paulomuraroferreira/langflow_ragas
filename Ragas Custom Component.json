{"data":{"edges":[],"nodes":[{"data":{"type":"RAGAS_Evaluation_Component","node":{"template":{"_type":"Component","context_chunks":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"context_chunks","display_name":"Retrieved chunks","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The chunks of text retrieved by the RAG","title_case":false,"type":"other"},"answer":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"answer","display_name":"Answer","advanced":false,"input_types":["Message"],"dynamic":false,"info":"RAG Generated answer","title_case":false,"type":"str"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nimport pandas as pd\nfrom datasets import Dataset\nfrom ragas import evaluate\nimport os\nfrom ragas.metrics import (\n    answer_relevancy,\n    faithfulness,\n    context_recall,\n    context_precision,\n)\nfrom langflow.schema.message import Message\n\nclass RAGAS_Evaluation_Component(Component):\n    display_name = \"Ragas Evaluator\"\n    description = \"Evaluates a question-answer pair using Ragas evaluation\"\n    icon = \"view\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"question\",\n            display_name=\"Question\",\n            info=\"RAG Question\",\n        ),\n        MessageTextInput(\n            name=\"answer\",\n            display_name=\"Answer\",\n            info=\"RAG Generated answer\",\n        ),\n        MessageTextInput(\n            name=\"ground_truth\",\n            display_name=\"Ground Truth (optional)\",\n            info=\"Ground Truth\",\n        ),\n        DataInput(\n            name=\"context_chunks\",\n            display_name=\"Retrieved chunks\",\n            info=\"The chunks of text retrieved by the RAG\",\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        \n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"ragas_evaluation\"),\n    ]\n        \n    def create_dataset(self, inputs: dict):\n        \n        self.data = {\n            \"question\": [inputs.get(\"question\", None)],\n            \"ground_truth\": [inputs.get(\"ground_truth\", None)],\n            \"answer\": [inputs.get(\"answer\", None)], \n            \"contexts\": [[data.data['text'] for data in inputs.get(\"context_data\", '')]]\n        }\n\n\n        self.df = pd.DataFrame(self.data)\n        \n        return self.df\n\n\n    def ragas_evaluation(self)-> Message:\n        \n        os.environ[\"OPENAI_API_KEY\"] = self.openai_api_key\n\n        self.evaluation_dict = {\"question\": self.question,\n                      \"answer\":self.answer,\n                      \"ground_truth\":self.ground_truth,\n                      \"context_data\":self.context_chunks}\n                      \n        if self.ground_truth:\n            metrics = [\n                faithfulness,\n                answer_relevancy,\n                context_recall,\n                context_precision\n            ]\n        else:\n            metrics = [\n                faithfulness,\n                answer_relevancy,\n                context_recall\n            ]\n\n        self.result = evaluate(\n            Dataset.from_pandas(self.create_dataset(self.evaluation_dict)),\n            metrics=metrics,\n        )\n        \n        df = self.result.to_pandas()\n        \n        question = df.loc[0,'question']\n        answer = df.loc[0,'answer']\n        faithfulness_ = df.loc[0,'faithfulness']\n        answer_relevancy_ = df.loc[0,'answer_relevancy']\n        context_recall_ = df.loc[0,'context_recall']\n        \n        string_to_return = f'Answer: {answer}\\nFaithfulness: {faithfulness_}\\nAnswer Relevancy: {answer_relevancy_}\\nContext Recall: {context_recall_}'\n        \n        if self.ground_truth:\n            context_precision_ = df.loc[0,'context_precision']\n            string_to_return += f'\\nContext Precision: {context_precision_}'\n            \n        return Message(text=string_to_return)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"ground_truth":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"ground_truth","display_name":"Ground Truth (optional)","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Ground Truth","title_case":false,"type":"str"},"openai_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"OpenAI API key","name":"openai_api_key","display_name":"OpenAI API Key","advanced":false,"input_types":[],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str"},"question":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"question","display_name":"Question","advanced":false,"input_types":["Message"],"dynamic":false,"info":"RAG Question","title_case":false,"type":"str"}},"description":"Evaluates a question-answer pair using Ragas evaluation","icon":"view","base_classes":["Message"],"display_name":"Custom Component","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"ragas_evaluation","value":"__UNDEFINED__","cache":true}],"field_order":["question","answer","ground_truth","context_chunks","openai_api_key"],"beta":false,"edited":true,"official":false},"id":"RAGAS_Evaluation_Component-OS0Uy","description":"Evaluates a question-answer pair using Ragas evaluation","display_name":"Custom Component"},"id":"RAGAS_Evaluation_Component-OS0Uy","position":{"x":0,"y":0},"type":"genericNode"}],"viewport":{"x":1,"y":1,"zoom":1}},"description":"Evaluates a question-answer pair using Ragas evaluation","name":"Custom Component","id":"RAGAS_Evaluation_Component-OS0Uy","is_component":true,"last_tested_version":"1.0.11"}